{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model as an Endpoint \n",
    "\n",
    "### What is an Endpoint? 🤔\n",
    "\n",
    "Azureml allows you to deploy models as endpoints. This means that you can send data to the endpoint and get a prediction back. In order to do this, you need to create a scoring script that will be used to make predictions, and an endpoint configuration that will define the compute resources that will be used to serve the endpoint. \n",
    "\n",
    "We will be making use of the `KubernetesOnlineEndpoint` class to deploy the model as an endpoint. This class will deploy the model to the already running K8s cluster. The class will also create a service that will expose the model as an endpoint. We use this instead of the `ManagedOnlineEndpoint` class because the `ManagedOnlineEndpoint` class will create Azure compute instances to serve the endpoint, which is more expensive and less performant than using the already running K8s cluster. \n",
    "\n",
    "### Local Deployment 🏠\n",
    "\n",
    "We will begin by deploying the model locally. This is useful for testing the endpoint before deploying it to the cloud. It is also useful for debugging the endpoint, and for testing the endpoint with a small amount of data. It will make the deployment process faster and easier.\n",
    "\n",
    "First, we will create the endpoint configuration. This will define the compute resources that will be used to serve the endpoint.\n",
    "\n",
    "**_Note: The local deployment will only work if you have Docker installed on your machine, and the Docker daemon is running._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    KubernetesOnlineEndpoint,\n",
    "    KubernetesOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential, ClientSecretCredential\n",
    "from azure.ai.ml.entities._deployment.resource_requirements_settings import (\n",
    "    ResourceRequirementsSettings,\n",
    ")\n",
    "from azure.ai.ml.entities._deployment.container_resource_settings import (\n",
    "    ResourceSettings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to the MLClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"0a94de80-6d3b-49f2-b3e9-ec5818862801\"\n",
    "resource_group = \"buas-y2\"\n",
    "workspace_name = \"Staff-Test\"\n",
    "tenant_id = \"0a33589b-0036-4fe8-a829-3ed0926af886\"\n",
    "client_id = \"a2230f31-0fda-428d-8c5c-ec79e91a49f5\"\n",
    "client_secret = \"Y-q8Q~H63btsUkR7dnmHrUGw2W0gMWjs0MxLKa1C\"\n",
    "\n",
    "credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the endpoint configuration for the local deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a local endpoint\n",
    "import datetime\n",
    "\n",
    "local_endpoint_name = \"local-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = KubernetesOnlineEndpoint(\n",
    "    name=local_endpoint_name, description=\"this is a sample local endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating local endpoint: local-06190730925377\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating local endpoint: {local_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install the docker package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docker in c:\\users\\deanv\\appdata\\roaming\\python\\python310\\site-packages (7.1.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\programdata\\anaconda3\\envs\\y2b\\lib\\site-packages (from docker) (305.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\envs\\y2b\\lib\\site-packages (from docker) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\programdata\\anaconda3\\envs\\y2b\\lib\\site-packages (from docker) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\y2b\\lib\\site-packages (from requests>=2.26.0->docker) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\y2b\\lib\\site-packages (from requests>=2.26.0->docker) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\y2b\\lib\\site-packages (from requests>=2.26.0->docker) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for tensorflow: [Errno 2] No such file or directory: 'c:\\\\programdata\\\\anaconda3\\\\envs\\\\y2b\\\\lib\\\\site-packages\\\\tensorflow-2.10.0.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "# Install docker package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a local endpoint, using the `local` deployment target. This will create a local endpoint that will run on your machine using Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating local endpoint (local-06190730925377) .Done (0m 5s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': None, 'scoring_uri': None, 'openapi_uri': None, 'name': 'local-06190730925377', 'description': 'this is a sample local endpoint', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': WindowsPath('C:/Users/deanv/.azureml/inferencing/local-06190730925377'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001A8E750E770>, 'auth_mode': 'key', 'location': None, 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now create the deployment config for the local endpoint. This will be used to create a service that will expose the model as an endpoint. You need to specfiy the model, the environment, the scoring script, and the endpoint configuration. The model needs to be a local model for this to work locally.\n",
    "\n",
    "**_Note: The environment and the scoring script are the main sources of error at this stage. You will need to make sure the environment has all the pre-requisites and that all of your path definitions for scoring are correct. Extensive logging is recommended for debugging._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.keras']\n",
      "c:\\Users\\deanv\\Dropbox\\0_Buas\\2023-2024\\y2D\\Azure Content Testing\\Example-App-master\\Example-App-master\\Notebooks\n",
      "['app.py', 'azure_utils', 'evaluate.py', 'load_data.py', 'model.py', 'models', 'predict.py', 'register.py', 'scoring.py', 'train.py', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "print(os.listdir('../models/download'))\n",
    "model = Model(path=\"../models/download/model.keras\")\n",
    "env = Environment(\n",
    "    conda_file=\"../../../example_conda.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/curated/tensorflow-2.16-cuda11:4\"#\"deanis/azure-gpu-inference\"#\"tensorflow/tensorflow:latest-gpu\"#\"mcr.microsoft.com/azureml/curated/tensorflow-2.16-cuda11:4\"#\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    ")\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir('../src/number_predictor'))\n",
    "\n",
    "blue_deployment = KubernetesOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=local_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../src/number_predictor\", scoring_script=\"scoring.py\"\n",
    "    ),\n",
    "    instance_count=1,\n",
    "    resources=ResourceRequirementsSettings(\n",
    "        requests=ResourceSettings(\n",
    "            cpu=\"100m\",\n",
    "            memory=\"0.5Gi\",\n",
    "            gpu=\"1\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, deploy the model as an endpoint, using the `local` flag. This will deploy the model to the local endpoint that was created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating local deployment (local-06190730925377 / blue) .\n",
      "Building Docker image from Dockerfile\n",
      "Step 1/6 : FROM mcr.microsoft.com/azureml/curated/tensorflow-2.16-cuda11:4\n",
      " ---> 6d0ec349c317\n",
      "Step 2/6 : RUN mkdir -p /var/azureml-app/\n",
      " ---> Using cache\n",
      " ---> a6fb8a24a661\n",
      "Step 3/6 : WORKDIR /var/azureml-app/\n",
      " ---> Using cache\n",
      " ---> 351579f60e7e\n",
      "Step 4/6 : COPY conda.yml /var/azureml-app/\n",
      " ---> Using cache\n",
      " ---> ea5d11993718\n",
      "Step 5/6 : RUN conda env create -n inf-conda-env --file conda.yml\n",
      " ---> Using cache\n",
      " ---> be1157b1d1bc\n",
      "Step 6/6 : CMD [\"conda\", \"run\", \"--no-capture-output\", \"-n\", \"inf-conda-env\", \"runsvdir\", \"/var/runit\"]\n",
      " ---> Using cache\n",
      " ---> f80d2ed2858f\n",
      "Successfully built f80d2ed2858f\n",
      "Successfully tagged local-06190730925377:blue\n",
      "\n",
      "Starting up endpoint...Done (0m 20s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KubernetesOnlineDeployment({'provisioning_state': 'Succeeded', 'endpoint_name': 'local-06190730925377', 'type': 'Kubernetes', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': WindowsPath('c:/Users/deanv/Dropbox/0_Buas/2023-2024/y2D/Azure Content Testing/Example-App-master/Example-App-master/Notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001A8E7579BA0>, 'model': Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': '6cb2a977a20c09f3db4caf0af16a5008', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': WindowsPath('c:/Users/deanv/Dropbox/0_Buas/2023-2024/y2D/Azure Content Testing/Example-App-master/Example-App-master/Notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001A8E757BC10>, 'version': '1', 'latest_version': None, 'path': 'C:\\\\Users\\\\deanv\\\\Dropbox\\\\0_Buas\\\\2023-2024\\\\y2D\\\\Azure Content Testing\\\\Example-App-master\\\\Example-App-master\\\\models\\\\download\\\\model.keras', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model', 'stage': None}), 'code_configuration': {'code': '../src/number_predictor'}, 'environment': Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/curated/tensorflow-2.16-cuda11:4', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'CliV2AnonymousEnvironment', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': WindowsPath('c:/Users/deanv/Dropbox/0_Buas/2023-2024/y2D/Azure Content Testing/Example-App-master/Example-App-master/Notebooks'), 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001A8E757BF40>, 'version': '1304d44e0b82dabb33fddbf355d93cee', 'conda_file': {'name': 'example_conda', 'channels': ['defaults'], 'dependencies': ['python=3.11', 'numpy', 'pandas', 'pip', {'pip': ['pillow', 'fastapi', 'uvicorn', 'azureml', 'azure-core', 'azure-identity', 'azureml-core', 'azure-ai-ml', 'mlflow', 'azureml-inference-server-http', 'azureml-defaults', 'azureml-mlflow', 'tensorflow']}]}, 'build': None, 'inference_config': None, 'os_type': None, 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': 'channels:\\n- defaults\\ndependencies:\\n- python=3.11\\n- numpy\\n- pandas\\n- pip\\n- pip:\\n  - pillow\\n  - fastapi\\n  - uvicorn\\n  - azureml\\n  - azure-core\\n  - azure-identity\\n  - azureml-core\\n  - azure-ai-ml\\n  - mlflow\\n  - azureml-inference-server-http\\n  - azureml-defaults\\n  - azureml-mlflow\\n  - tensorflow\\nname: example_conda\\n'}), 'environment_variables': {}, 'app_insights_enabled': False, 'scale_settings': None, 'request_settings': None, 'liveness_probe': None, 'readiness_probe': None, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'local', 'data_collector': None, 'resources': <azure.ai.ml.entities._deployment.resource_requirements_settings.ResourceRequirementsSettings object at 0x000001A8E757BC40>})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(\n",
    "    deployment=blue_deployment, local=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can check the status of the deployment by looking at the logs of the docker container that was created. In docker desktop, you can do this by clicking on the container, and then clicking on the logs tab. The container will be named something like `local-06160804621045.blue`.\n",
    "\n",
    "- You can also check the status of the deployment by looking at the logs of the deployment with the `ml_client`. You can do this by calling the `get_logs` method on the deployment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth_mode: key\n",
      "description: this is a sample local endpoint\n",
      "location: local\n",
      "mirror_traffic: {}\n",
      "name: local-06190730925377\n",
      "properties: {}\n",
      "provisioning_state: Succeeded\n",
      "scoring_uri: http://localhost:32776/score\n",
      "tags: {}\n",
      "traffic: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = ml_client.online_endpoints.get(name=local_endpoint_name, local=True)\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 11.8.0\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\n",
      "\n",
      "2024-06-19T05:30:37,166462030+00:00 - rsyslog/run \n",
      "2024-06-19T05:30:37,178807446+00:00 - gunicorn/run \n",
      "2024-06-19T05:30:37,183517686+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:37,184568390+00:00 - nginx/run \n",
      "2024-06-19T05:30:37,187054749+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:37,189716981+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2024-06-19T05:30:37,191655386+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:37,193924974+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:37,531102767+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:37,534464998+00:00 | gunicorn/run | AzureML image information: openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04, Materializaton Build:20240603.v1\n",
      "2024-06-19T05:30:37,536891190+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:37,538644410+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:37,540180891+00:00 | gunicorn/run | PATH environment variable: /opt/miniconda/envs/inf-conda-env/bin:/opt/miniconda/condabin:/azureml-envs/tensorflow-2.16-cuda11/bin:/opt/miniconda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2024-06-19T05:30:37,541384444+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2024-06-19T05:30:37,543122094+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:37,842870429+00:00 | gunicorn/run | CONDAPATH environment variable: /opt/miniconda\n",
      "\n",
      "# conda environments:\n",
      "#\n",
      "                         /azureml-envs/tensorflow-2.16-cuda11\n",
      "base                     /opt/miniconda\n",
      "inf-conda-env         *  /opt/miniconda/envs/inf-conda-env\n",
      "\n",
      "2024-06-19T05:30:38,353642328+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:38,354590207+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "absl-py==2.1.0\n",
      "adal==1.2.7\n",
      "alembic==1.13.1\n",
      "aniso8601==9.0.1\n",
      "annotated-types==0.7.0\n",
      "anyio==4.4.0\n",
      "argcomplete==3.4.0\n",
      "astunparse==1.6.3\n",
      "attrs==23.2.0\n",
      "azure-ai-ml==1.16.1\n",
      "azure-common==1.1.28\n",
      "azure-core==1.30.2\n",
      "azure-graphrbac==0.61.1\n",
      "azure-identity==1.16.1\n",
      "azure-mgmt-authorization==4.0.0\n",
      "azure-mgmt-containerregistry==10.3.0\n",
      "azure-mgmt-core==1.4.0\n",
      "azure-mgmt-keyvault==10.3.0\n",
      "azure-mgmt-network==25.4.0\n",
      "azure-mgmt-resource==23.1.1\n",
      "azure-mgmt-storage==21.1.0\n",
      "azure-storage-blob==12.19.0\n",
      "azure-storage-file-datalake==12.14.0\n",
      "azure-storage-file-share==12.16.0\n",
      "azureml==0.2.7\n",
      "azureml-core==1.56.0\n",
      "azureml-dataprep==5.1.6\n",
      "azureml-dataprep-native==41.0.0\n",
      "azureml-dataprep-rslex==2.22.2\n",
      "azureml-dataset-runtime==1.56.0\n",
      "azureml-defaults==1.56.0.post1\n",
      "azureml-inference-server-http==1.2.2\n",
      "azureml-mlflow==1.56.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==4.1.3\n",
      "blinker==1.8.2\n",
      "Bottleneck @ file:///croot/bottleneck_1707864210935/work\n",
      "cachetools==5.3.3\n",
      "certifi==2024.6.2\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "cloudpickle==2.2.1\n",
      "colorama==0.4.6\n",
      "contextlib2==21.6.0\n",
      "contourpy==1.2.1\n",
      "cryptography==42.0.8\n",
      "cycler==0.12.1\n",
      "Deprecated==1.2.14\n",
      "dnspython==2.6.1\n",
      "docker==7.1.0\n",
      "email_validator==2.1.1\n",
      "entrypoints==0.4\n",
      "fastapi==0.111.0\n",
      "fastapi-cli==0.0.4\n",
      "Flask==2.3.2\n",
      "Flask-Cors==3.0.10\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.53.0\n",
      "fusepy==3.0.1\n",
      "gast==0.5.4\n",
      "gitdb==4.0.11\n",
      "GitPython==3.1.43\n",
      "google-api-core==2.19.0\n",
      "google-auth==2.30.0\n",
      "google-pasta==0.2.0\n",
      "googleapis-common-protos==1.63.1\n",
      "graphene==3.3\n",
      "graphql-core==3.2.3\n",
      "graphql-relay==3.2.0\n",
      "greenlet==3.0.3\n",
      "grpcio==1.64.1\n",
      "gunicorn==22.0.0\n",
      "h11==0.14.0\n",
      "h5py==3.11.0\n",
      "httpcore==1.0.5\n",
      "httptools==0.6.1\n",
      "httpx==0.27.0\n",
      "humanfriendly==10.0\n",
      "idna==3.7\n",
      "importlib_metadata==7.1.0\n",
      "inference-schema==1.7.2\n",
      "isodate==0.6.1\n",
      "itsdangerous==2.2.0\n",
      "jeepney==0.8.0\n",
      "Jinja2==3.1.4\n",
      "jmespath==1.0.1\n",
      "joblib==1.4.2\n",
      "jsonpickle==3.2.1\n",
      "jsonschema==4.22.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "keras==3.3.3\n",
      "kiwisolver==1.4.5\n",
      "knack==0.11.0\n",
      "libclang==18.1.1\n",
      "Mako==1.3.5\n",
      "Markdown==3.6\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.5\n",
      "marshmallow==3.21.3\n",
      "matplotlib==3.9.0\n",
      "mdurl==0.1.2\n",
      "mkl-fft @ file:///croot/mkl_fft_1695058164594/work\n",
      "mkl-random @ file:///croot/mkl_random_1695059800811/work\n",
      "mkl-service==2.4.0\n",
      "ml-dtypes==0.3.2\n",
      "mlflow==2.13.2\n",
      "mlflow-skinny==2.13.2\n",
      "msal==1.28.1\n",
      "msal-extensions==1.1.0\n",
      "msrest==0.7.1\n",
      "msrestazure==0.6.4\n",
      "namex==0.0.8\n",
      "ndg-httpsclient==0.5.1\n",
      "numexpr @ file:///croot/numexpr_1696515281613/work\n",
      "numpy==1.23.5\n",
      "oauthlib==3.2.2\n",
      "opencensus==0.11.4\n",
      "opencensus-context==0.1.3\n",
      "opencensus-ext-azure==1.1.13\n",
      "opencensus-ext-logging==0.1.1\n",
      "opentelemetry-api==1.25.0\n",
      "opentelemetry-sdk==1.25.0\n",
      "opentelemetry-semantic-conventions==0.46b0\n",
      "opt-einsum==3.3.0\n",
      "optree==0.11.0\n",
      "orjson==3.10.5\n",
      "packaging==24.1\n",
      "pandas @ file:///croot/pandas_1718308974269/work/dist/pandas-2.2.2-cp311-cp311-linux_x86_64.whl#sha256=3c7ce50f9f519c785bd4cdb28a0ca71f85a541f3d27b25aa9da770f953e7f2e9\n",
      "paramiko==3.4.0\n",
      "pathspec==0.12.1\n",
      "pillow==10.3.0\n",
      "pkginfo==1.11.1\n",
      "portalocker==2.8.2\n",
      "proto-plus==1.23.0\n",
      "protobuf==4.25.3\n",
      "psutil==5.9.8\n",
      "pyarrow==15.0.2\n",
      "pyasn1==0.6.0\n",
      "pyasn1_modules==0.4.0\n",
      "pycparser==2.22\n",
      "pydantic==2.7.4\n",
      "pydantic-settings==2.3.3\n",
      "pydantic_core==2.18.4\n",
      "pydash==8.0.1\n",
      "Pygments==2.18.0\n",
      "PyJWT==2.8.0\n",
      "PyNaCl==1.5.0\n",
      "pyOpenSSL==24.1.0\n",
      "pyparsing==3.1.2\n",
      "PySocks==1.7.1\n",
      "python-dateutil @ file:///croot/python-dateutil_1716495738603/work\n",
      "python-dotenv==1.0.1\n",
      "python-multipart==0.0.9\n",
      "pytz @ file:///croot/pytz_1713974312559/work\n",
      "PyYAML==6.0.1\n",
      "querystring-parser==1.2.4\n",
      "referencing==0.35.1\n",
      "requests==2.32.3\n",
      "requests-oauthlib==2.0.0\n",
      "rich==13.7.1\n",
      "rpds-py==0.18.1\n",
      "rsa==4.9\n",
      "scikit-learn==1.5.0\n",
      "scipy==1.13.1\n",
      "SecretStorage==3.3.3\n",
      "shellingham==1.5.4\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "smmap==5.0.1\n",
      "sniffio==1.3.1\n",
      "SQLAlchemy==2.0.30\n",
      "sqlparse==0.5.0\n",
      "starlette==0.37.2\n",
      "strictyaml==1.7.3\n",
      "tabulate==0.9.0\n",
      "tensorboard==2.16.2\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.16.1\n",
      "tensorflow-io-gcs-filesystem==0.37.0\n",
      "termcolor==2.4.0\n",
      "threadpoolctl==3.5.0\n",
      "tqdm==4.66.4\n",
      "typer==0.12.3\n",
      "typing_extensions==4.12.2\n",
      "tzdata @ file:///croot/python-tzdata_1690578112552/work\n",
      "ujson==5.10.0\n",
      "urllib3==2.2.1\n",
      "uvicorn==0.30.1\n",
      "uvloop==0.19.0\n",
      "watchfiles==0.22.0\n",
      "websockets==12.0\n",
      "Werkzeug==3.0.3\n",
      "wrapt==1.16.0\n",
      "zipp==3.19.2\n",
      "\n",
      "2024-06-19T05:30:39,853655342+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:39,855158322+00:00 | gunicorn/run | Entry script directory: /var/azureml-app/number_predictor//.\n",
      "2024-06-19T05:30:39,856379297+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:39,857462974+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:39,858555637+00:00 | gunicorn/run | Dynamic Python Package Installation\n",
      "2024-06-19T05:30:39,859861141+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:39,860717697+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:39,861626725+00:00 | gunicorn/run | Dynamic Python package installation is disabled.\n",
      "2024-06-19T05:30:39,862507340+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:39,863391268+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:39,864438452+00:00 | gunicorn/run | Checking if the Python package azureml-inference-server-http is installed\n",
      "2024-06-19T05:30:39,865461497+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:39,866364899+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:41,024993827+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:41,026618617+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:41,028213949+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2024-06-19T05:30:41,030283318+00:00 | gunicorn/run | ###############################################\n",
      "2024-06-19T05:30:41,031138415+00:00 | gunicorn/run | \n",
      "2024-06-19T05:30:41,034121494+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "2024-06-19 05:30:41,203 I [50] azmlinfsrv - Loaded logging config from /opt/miniconda/envs/inf-conda-env/lib/python3.11/site-packages/azureml_inference_server_http/logging.json\n",
      "\n",
      "Azure ML Inferencing HTTP server v1.2.2\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/number_predictor/scoring.py\n",
      "Model Directory: /var/azureml-app/azureml-models//6cb2a977a20c09f3db4caf0af16a5008/1\n",
      "Config File: None\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Health Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/1.2.2\n",
      "CORS for the specified origins: None\n",
      "Create dedicated endpoint for health: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "\n",
      "Warnings\n",
      "---------------\n",
      "Azmlinfsrv will be migrating to Pydantic 2.0 on 1/15/24. This is a breaking change for any Pydantic 1.0 code.\n",
      "\n",
      "2024-06-19 05:30:41,242 I [50] gunicorn.error - Starting gunicorn 22.0.0\n",
      "2024-06-19 05:30:41,244 I [50] gunicorn.error - Listening at: http://0.0.0.0:31311 (50)\n",
      "2024-06-19 05:30:41,244 I [50] gunicorn.error - Using worker: sync\n",
      "2024-06-19 05:30:41,248 I [130] gunicorn.error - Booting worker with pid: 130\n",
      "/opt/miniconda/envs/inf-conda-env/lib/python3.11/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_dc_storage_enabled\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "2024-06-19 05:30:41,464 W [130] azmlinfsrv - Found extra keys in the config file that are not supported by the server.\n",
      "Extra keys = ['AZUREML_ENTRY_SCRIPT', 'AZUREML_MODEL_DIR', 'HOSTNAME']\n",
      "2024-06-19 05:30:42,176 I [130] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "Initializing logger\n",
      "2024-06-19 05:30:42,184 I [130] azmlinfsrv - Starting up app insights client\n",
      "2024-06-19 05:30:44.387067: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-19 05:30:44.397118: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-19 05:30:44.651441: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-19 05:30:45.743623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 05:30:47.573236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-19 05:30:50,536 I [130] azmlinfsrv.user_script - Found user script at /var/azureml-app/number_predictor/scoring.py\n",
      "2024-06-19 05:30:50,536 I [130] azmlinfsrv.user_script - run() is not decorated. Server will invoke it with the input in JSON string.\n",
      "2024-06-19 05:30:50,536 I [130] azmlinfsrv.user_script - Invoking user's init function\n",
      "WARNING:tensorflow:From /var/azureml-app/number_predictor/scoring.py:15: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "2024-06-19 05:30:50,566 I [130] azmlinfsrv.print - The CPU will be used for scoring.\n",
      "2024-06-19 05:30:50,568 I [130] azmlinfsrv.print - Gpus: []\n",
      "2024-06-19 05:30:50,568 I [130] azmlinfsrv.print - No GPU found :(\n",
      "2024-06-19 05:30:50,568 I [130] azmlinfsrv.print - Could not set memory growth for GPU\n",
      "2024-06-19 05:30:50,568 I [130] azmlinfsrv.print - base_path: /var/azureml-app/azureml-models//6cb2a977a20c09f3db4caf0af16a5008/1\n",
      "2024-06-19 05:30:50,568 I [130] azmlinfsrv.print - list files in the model_path directory\n",
      "2024-06-19 05:30:50,571 I [130] azmlinfsrv.print - 1/\n",
      "2024-06-19 05:30:50,572 I [130] azmlinfsrv.print -     model.keras\n",
      "2024-06-19 05:30:50,572 I [130] azmlinfsrv.print - model_path: /var/azureml-app/azureml-models//6cb2a977a20c09f3db4caf0af16a5008/1/model.keras\n",
      "2024-06-19 05:30:51,142 I [130] azmlinfsrv.print - Model loaded successfully\n",
      "2024-06-19 05:30:51,142 I [130] azmlinfsrv.user_script - Users's init has completed successfully\n",
      "2024-06-19 05:30:51,144 I [130] azmlinfsrv.swagger - Swaggers are prepared for the following versions: [2, 3, 3.1].\n",
      "2024-06-19 05:30:51,144 I [130] azmlinfsrv - Scoring timeout is set to 3600000\n",
      "2024-06-19 05:30:51,144 I [130] azmlinfsrv - Worker with pid 130 ready for serving traffic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=local_endpoint_name, local=True, lines=500\n",
    ")\n",
    "\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Testing 🧪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the deployment is successful, you can test the endpoint by sending data to it. We will do this by sending a request to the endpoint using the `invoke` method on the deployment object. This will send a request to the endpoint, and return the response. The format of the request and response will depend on the scoring script that was used to create the endpoint. In this case the scoring script is `scoring.py`, which expects a JSON object with a key `data` that contains the data to be scored. The data is a base64 encoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"4\"'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=local_endpoint_name,\n",
    "    request_file=\"sample-request4.json\",\n",
    "    local=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Deployment 🌐\n",
    "\n",
    "Once the local deployment is successful, we can deploy the model to the cloud. This is useful for serving the model to a large number of users, and for making the model accessible from anywhere. It is also useful for deploying the model to a production environment.\n",
    "\n",
    "- First we need to create the endpoint configuration for the cloud deployment. This will define thename, authorisation method, and compute resources that will be used to serve the endpoint. We will use the `KubernetesOnlineEndpoint` class to deploy the model to the already running K8s cluster. This class will create a service that will expose the model as an endpoint. We use this instead of the `ManagedOnlineEndpoint` class because the `ManagedOnlineEndpoint` class will create Azure compute instances to serve the endpoint, which is more expensive and less performant than using the already running K8s cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"k8s-endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = KubernetesOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    compute=\"adsai1\",\n",
    "    description=\"this is a sample online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"Type\": \"Review Session\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KubernetesOnlineEndpoint({'provisioning_state': 'Succeeded', 'scoring_uri': 'http://194.171.191.227:30397/api/v1/endpoint/k8s-endpoint-06190717733661/score', 'openapi_uri': 'http://194.171.191.227:30397/api/v1/endpoint/k8s-endpoint-06190717733661/swagger.json', 'name': 'k8s-endpoint-06190717733661', 'description': 'this is a sample online endpoint', 'tags': {'Type': 'Review Session'}, 'properties': {'createdBy': 'a2230f31-0fda-428d-8c5c-ec79e91a49f5', 'createdAt': '2024-06-19T05:17:22.786982+0000', 'lastModifiedAt': '2024-06-19T05:17:22.786982+0000', 'azureml.onlineendpointid': '/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/providers/microsoft.machinelearningservices/workspaces/staff-test/onlineendpoints/k8s-endpoint-06190717733661', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/providers/Microsoft.MachineLearningServices/locations/westeurope/mfeOperationsStatus/oeidp:798f953a-277e-4ed7-90e2-0e1cd6d888eb:2238843d-c73a-4575-b2ec-151d991d20f7?api-version=2022-02-01-preview'}, 'print_as_yaml': False, 'id': '/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourceGroups/buas-y2/providers/Microsoft.MachineLearningServices/workspaces/Staff-Test/onlineEndpoints/k8s-endpoint-06190717733661', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\deanv\\\\Dropbox\\\\0_Buas\\\\2023-2024\\\\y2D\\\\Azure Content Testing\\\\Example-App-master\\\\Example-App-master\\\\Notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001A8E40442B0>, 'auth_mode': 'key', 'location': 'westeurope', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x000001A8E3CE2B30>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'K8S', 'compute': '/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourceGroups/buas-y2/providers/Microsoft.MachineLearningServices/workspaces/Staff-Test/computes/adsai1'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we need to create the deployment config for the cloud deployment. This will be used to create a service that will expose the model as an endpoint. You need to specfiy the model, the environment, the scoring script, and the endpoint configuration. We can now use registered models (and environments) for this deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\Dropbox\\0_Buas\\2023-2024\\y2D\\Azure Content Testing\\Example-App-master\\Example-App-master\\Notebooks\n",
      "['app.py', 'azure_utils', 'evaluate.py', 'load_data.py', 'model.py', 'models', 'predict.py', 'register.py', 'scoring.py', 'train.py', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "env = Environment(\n",
    "    # conda_file=\"../../../example_conda.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/curated/tensorflow-2.16-cuda11:4\"#\"deanis/azure-gpu-inference\"#\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    ")\n",
    "\n",
    "registered_model_name = \"example_2\"\n",
    "latest_model_version = 2\n",
    "registered_environment_name = \"endpoint_env_inference\"\n",
    "latest_environment_version = 2\n",
    "\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "# env = ml_client.environments.get(name=registered_environment_name, version=latest_environment_version)\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir('../src/number_predictor'))\n",
    "\n",
    "blue_deployment = KubernetesOnlineDeployment(\n",
    "    name=\"green\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"../src/number_predictor\", scoring_script=\"scoring.py\"\n",
    "    ),\n",
    "    instance_count=1,\n",
    "    resources=ResourceRequirementsSettings(\n",
    "        requests=ResourceSettings(\n",
    "            cpu=\"100m\",\n",
    "            memory=\"0.5Gi\",\n",
    "        ),\n",
    "        # limits=ResourceSettings(\n",
    "        #     cpu=\"1\",\n",
    "        #     memory=\"2Gi\",\n",
    "        #     gpu=1,\n",
    "        # ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now deploy the model as an endpoint, **without** the `local` flag. This will deploy the model to the cloud endpoint that was created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint k8s-endpoint-06190717733661 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................."
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "(None) KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg\nCode: None\nMessage: KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg\nException Details:\t(None) KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg\n\tCode: None\n\tMessage: KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationFailed\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\polling\\base_polling.py:757\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 757\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadStatus \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\polling\\base_polling.py:789\u001b[0m, in \u001b[0;36mLROBasePolling._poll\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _failed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus()):\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationFailed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation failed or canceled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    791\u001b[0m final_get_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39mget_final_get_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response)\n",
      "\u001b[1;31mOperationFailed\u001b[0m: Operation failed or canceled",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblue_deployment\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\polling\\_poller.py:251\u001b[0m, in \u001b[0;36mLROPoller.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PollingReturnType_co:\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the result of the long running operation, or\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m    the result available after the specified timeout.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    :raises ~azure.core.exceptions.HttpResponseError: Server problem with the query.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_polling_method\u001b[38;5;241m.\u001b[39mresource()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\polling\\_poller.py:270\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# Was None\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\polling\\_poller.py:185\u001b[0m, in \u001b[0;36mLROPoller._start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start the long running operation.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mOn completion, runs any callbacks.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_polling_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\azure\\core\\polling\\base_polling.py:772\u001b[0m, in \u001b[0;36mLROBasePolling.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(\n\u001b[0;32m    766\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response,\n\u001b[0;32m    767\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(err),\n\u001b[0;32m    768\u001b[0m         error\u001b[38;5;241m=\u001b[39merr,\n\u001b[0;32m    769\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailed \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 772\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response, error\u001b[38;5;241m=\u001b[39merr) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (None) KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg\nCode: None\nMessage: KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg\nException Details:\t(None) KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg\n\tCode: None\n\tMessage: KubernetesOperationError: Operation failed in Kubernetes cluster. Reason:UserScriptInitFailed Details:Errors Occurred in User Script init fuction, please check the deployment logs for more details. Please see troubleshooting guide, available here: https://aka.ms/amlarc-tsg"
     ]
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The last step is to route traffic to the deploymennt. We only have 1 deployment (blue), so we can route all traffic to it. We can do this by calling the `endpoint.traffic` method on the endpoint object, and passing in the percentage of traffic that should be routed to the deployment. In this case we will route 100% of the traffic to the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KubernetesOnlineEndpoint({'provisioning_state': 'Succeeded', 'scoring_uri': 'http://194.171.191.227:30397/api/v1/endpoint/k8s-endpoint-06161121150181/score', 'openapi_uri': 'http://194.171.191.227:30397/api/v1/endpoint/k8s-endpoint-06161121150181/swagger.json', 'name': 'k8s-endpoint-06161121150181', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar', 'baz': 'qux', 'quux': 'corge'}, 'properties': {'createdBy': 'a2230f31-0fda-428d-8c5c-ec79e91a49f5', 'createdAt': '2024-06-16T09:21:59.809151+0000', 'lastModifiedAt': '2024-06-16T09:21:59.809151+0000', 'azureml.onlineendpointid': '/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourcegroups/buas-y2/providers/microsoft.machinelearningservices/workspaces/staff-test/onlineendpoints/k8s-endpoint-06161121150181', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/providers/Microsoft.MachineLearningServices/locations/westeurope/mfeOperationsStatus/oeidp:798f953a-277e-4ed7-90e2-0e1cd6d888eb:c90d788c-45c8-4966-a65a-ae4d4876a647?api-version=2022-02-01-preview'}, 'print_as_yaml': False, 'id': '/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourceGroups/buas-y2/providers/Microsoft.MachineLearningServices/workspaces/Staff-Test/onlineEndpoints/k8s-endpoint-06161121150181', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\deanv\\\\Dropbox\\\\0_Buas\\\\2023-2024\\\\y2D\\\\Azure Content Testing\\\\Example-App-master\\\\Example-App-master\\\\Notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001C0197D57F0>, 'auth_mode': 'key', 'location': 'westeurope', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x000001C0183A72C0>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'K8S', 'compute': '/subscriptions/0a94de80-6d3b-49f2-b3e9-ec5818862801/resourceGroups/buas-y2/providers/Microsoft.MachineLearningServices/workspaces/Staff-Test/computes/adsai1'})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can check the status of the deployment by looking at the logs of the deployment with the `ml_client`. You can do this by calling the `get_logs` method on the deployment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-16 09:50:37,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:50:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:50:37,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:50:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:50:47,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:50:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:50:47,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:50:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:50:57,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:50:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:50:57,083 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:50:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:07,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:07,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:17,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:17,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:27,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:27,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:37,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:37,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:47,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:47,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:57,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:51:57,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:51:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:07,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:07,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:17,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:17,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:27,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:27,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:37,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:37,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:47,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:47,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:57,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:52:57,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:52:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:07,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:07,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:17,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:17,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:27,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:27,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:37,075 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:37,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:47,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:47,078 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:47 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:57,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:53:57,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:53:57 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:07,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:07,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:07 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:17,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:17,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:17 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:27,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:27,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:27 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:37,076 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n2024-06-16 09:54:37,077 I [326] gunicorn.access - 127.0.0.1 - - [16/Jun/2024:09:54:37 +0000] \"GET / HTTP/1.0\" 200 7 \"-\" \"kube-probe/1.29\"\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    ")\n",
    "\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue': 100}\n",
      "http://194.171.191.227:30397/api/v1/endpoint/k8s-endpoint-06161121150181/score\n"
     ]
    }
   ],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Cloud Endpoint and Blue Deployment🧪\n",
    "\n",
    "The scoring script for the cloud deployment is the same as the scoring script for the local deployment. It expects a JSON object with a key `data` that contains the data to be scored. The data is a base64 encoded image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# load image and encode it in base64\n",
    "import base64\n",
    "import json\n",
    "\n",
    "image_path = \"../data/MNIST_44_0.png\"\n",
    "# image_path = \"../data/test_image5.png\"\n",
    "with open(image_path, \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "print(base64_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the deployment in AzureML studio, you can see the status of the deployment, and the logs of the deployment. You can also see the traffic that is being routed to the deployment. If you click on the `consume` tab, you can see the code that you need to use to send data to the endpoint. You can also see the response that you get back from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"7\"'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data = {\n",
    "    \"data\": base64_image\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "#url = 'http://194.171.191.227:30397/api/v1/endpoint/k8s-endpoint-06161121150181/score'\n",
    "url = 'https://6c68-194-171-191-227.ngrok-free.app/api/v1/endpoint/k8s-endpoint-06161121150181/score'\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = 'tX5Xv8Lomc7BrhBWcVzpY6elCAtVPUcR'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU NEED TO INSTALL RUNIT!!!, or use an azure image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
