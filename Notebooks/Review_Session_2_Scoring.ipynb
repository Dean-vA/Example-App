{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scoring Script üìù\n",
    "\n",
    "When deploying a model as an endpoint in Azure Machine Learning, you need to provide a scoring script that will be used to make predictions. This script should contain two required functions:\n",
    "\n",
    "1. `init()`: This function loads the model into memory when the service starts.\n",
    "2. `run(input_data)`: This function uses the model to predict a value based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_**: The `init()` and `run(input_data)` functions are required for the scoring script to work. You can add additional functions to the script as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `init()` Function üèÅ\n",
    "\n",
    "- The `init()` function is called when the service is initialized. \n",
    "\n",
    "- In this function, you should load the model into memory so that it can be used for scoring. The path to the model file is provided by the `Model` object that is passed to the `init()` function as an input when you deploy the model to an endpoint. \n",
    "\n",
    "- Extensive logging is recommended in this function to help find the correct path to the model file and diagnose any potential issues that may arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def init():\n",
    "    # Define the model as a global variable to be used later in the predict function\n",
    "    global model\n",
    "\n",
    "    # Get the path where the model is saved, it is set in the environment variable AZUREML_MODEL_DIR by the deployment configuration\n",
    "    base_path = os.getenv(\"AZUREML_MODEL_DIR\")\n",
    "    print(f\"base_path: {base_path}\")\n",
    "    \n",
    "    # show the files in the model_path directory\n",
    "    print(f\"list files in the model_path directory\")\n",
    "    # list files and dirs in the model_path directory\n",
    "    list_files(base_path)\n",
    "    \n",
    "    # add the model file name to the base_path\n",
    "    model_path = os.path.join(base_path, 'model.keras') # local\n",
    "    # model_path = os.path.join(base_path, \"INPUT_model\", 'model.keras') # azure\n",
    "    # print the model_path to check if it is correct\n",
    "    print(f\"model_path: {model_path}\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "# Helper function to list files in a directory\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, \"\").count(os.sep)\n",
    "        indent = \" \" * 4 * (level)\n",
    "        print(\"{}{}/\".format(indent, os.path.basename(root)))\n",
    "        subindent = \" \" * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(\"{}{}\".format(subindent, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `run(input_data)` Function üèÉ\n",
    "\n",
    "- The `run(input_data)` function is called when the endpoint is hit with an HTTP request.\n",
    "- The input data is passed to this function as a JSON payload with the following format:\n",
    " `{\"data\": \"base64-encoded-image-string\"}`.\n",
    "- We need to:\n",
    "    - Decode the input data from base64.\n",
    "    - Preprocess the input data.\n",
    "    - Use the model to predict the class of the input image.\n",
    "    - Postprocess the prediction (if necessary)\n",
    "    - Return the prediction as a JSON payload with the following format:\n",
    "    `{\"result\": \"predicted-class\"}`.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note_**: It is advised to test this with a local deployment before deploying the model as an endpoint in Azure Machine Learning. That way you can build and debug the scoring incrementally using the logs to understand the flow of data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    # Load the JSON data from the POST request, print the data to see the structure and content\n",
    "    print(f\"raw_data: {raw_data}\")\n",
    "    data = json.loads(raw_data)\n",
    "    print(f\"data: {data}\")\n",
    "\n",
    "    # Get the base64-encoded image data, print the data to see make sure it is correct\n",
    "    base64_image = data[\"data\"]\n",
    "    print(f\"base64_image: {base64_image}\")\n",
    "\n",
    "    # Decode the base64 string into bytes\n",
    "    image_bytes = base64.b64decode(base64_image)\n",
    "    print(f\"image_bytes: {image_bytes}\")\n",
    "\n",
    "    # Open the bytes as an image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Preprocess the image\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert(\"L\")\n",
    "    # Resize the image to 28x28 pixels, the size expected by the model\n",
    "    image = image.resize((28, 28))\n",
    "    # Convert the image to a numpy array and normalize pixel values to [0, 1]\n",
    "    data = np.array(image) / 255.0\n",
    "    # The model expects a 4D tensor of shape (batch_size, height, width, channels),\n",
    "    # so we add an extra dimension to the start and end of the array\n",
    "    data = np.expand_dims(data, axis=(0, -1))\n",
    "\n",
    "    # Make prediction, print the prediction to see the structure and content\n",
    "    prediction = model.predict(data)\n",
    "    print(f\"prediction: {prediction}\")\n",
    "    # Get the predicted label\n",
    "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    # Print the predicted label\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print(f\"Output format: {json.dumps(predicted_label.tolist())}\")\n",
    "\n",
    "    return json.dumps(predicted_label.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y2b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
